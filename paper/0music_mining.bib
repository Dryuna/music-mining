Automatically generated by Mendeley Desktop 1.16
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Guardia-Sebaoun2015,
abstract = {For recommender systems, time is often an important source of information but it is also a complex dimension to apprehend. We propose here to learn item and user representations such that any timely ordered sequence of items selected by a user will be repre- sented as a trajectory of the user in a representation space. This allows us to rank new items for this user. We then enrich the item and user representations in order to perform rating prediction using a classical matrix factorization scheme. We demonstrate the interest of our approach regarding both item ranking and rating prediction on a series of classical benchmarks.},
author = {Guardia-Sebaoun, Elie and Guigue, Vincent and Gallinari, Patrick},
doi = {10.1145/2792838.2799676},
file = {:Users/jaanaltosaar/papers/mendeley collection/Guardia-Sebaoun, Guigue, Gallinari - 2015 - Latent Trajectory Modeling A Light and Efficient Way to Introduce Time in Recommender Syste.pdf:pdf},
isbn = {9781450336925},
journal = {the 2015 ACM conference on Recommender systems, RecSys 2015},
pages = {281--284},
title = {{Latent Trajectory Modeling : A Light and Efficient Way to Introduce Time in Recommender Systems}},
url = {http://www-connex.lip6.fr/{~}guigue/wikihomepage/uploads/Research/recsys15.pdf},
year = {2015}
}
@article{Blei2003,
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
file = {:Users/jaanaltosaar/papers/mendeley collection/Blei, Ng, Jordan - 2003 - Latent Dirichlet Allocation.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
url = {http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf},
volume = {3},
year = {2003}
}
@article{Lei2015,
abstract = {The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting convolution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance, we obtain 51.4{\%} accuracy on the fine-grained sentiment classification task.},
annote = {via alp},
archivePrefix = {arXiv},
arxivId = {1508.04112},
author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
eprint = {1508.04112},
file = {:Users/jaanaltosaar/papers/mendeley collection/Lei, Barzilay, Jaakkola - 2015 - Molding CNNs for text non-linear, non-consecutive convolutions.pdf:pdf},
title = {{Molding CNNs for text: non-linear, non-consecutive convolutions}},
url = {http://arxiv.org/abs/1508.04112},
year = {2015}
}
@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:Users/jaanaltosaar/papers/mendeley collection/Mikolov et al. - 2013 - Distributed representations of words and phrases and their compositionality.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
title = {{Distributed representations of words and phrases and their compositionality}},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
year = {2013}
}
@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
eprint = {arXiv:1301.3781v3},
file = {:Users/jaanaltosaar/papers/mendeley collection/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
journal = {ICLR},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
@article{Arora2015,
abstract = {Semantic word embeddings use vector representations to represent the meaning of a word. Methods to create them include Vector Space Methods (VSMs) such as Latent Semantic Analysis (LSA), matrix factorization, generative text models such as Topic Models, and neural nets. A flurry of work has resulted from the papers of Mikolov et al.{\~{}}$\backslash$cite{\{}mikolov2013efficient{\}}. These showed how to solve word analogy tasks very well by leveraging linear structure in word embeddings even though the embeddings were created using highly nonlinear energy based models. No clear explanation is known why such linear structure emerges in low-dimensional embeddings. This paper presents a loglinear generative model---related to{\~{}}$\backslash$citet{\{}mnih2007three{\}}---that models the generation of a text corpus as a random walk in a latent discourse space. A novel methodological twist is that the model is solved in closed form by integrating out the random walk. This yields a simple method for constructing word embeddings. Experiments are presented to support the modeling assumptions as well as the efficacy of the word embeddings for solving analogies. This simple model links and provides theoretical support for several prior methods for finding embeddings, as well as provides interpretations for various linear algebraic structures in word embeddings obtained from nonlinear techniques.},
archivePrefix = {arXiv},
arxivId = {1502.03520},
author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
eprint = {1502.03520},
file = {:Users/jaanaltosaar/papers/mendeley collection/Arora et al. - 2015 - Random walks on discourse spaces a new generative language model with applications to semantic word embeddings.pdf:pdf},
pages = {1--23},
title = {{Random walks on discourse spaces: a new generative language model with applications to semantic word embeddings}},
url = {http://arxiv.org/abs/1502.03520},
year = {2015}
}
@article{Pennington,
author = {Pennington, Jeffrey and Manning, C},
file = {:Users/jaanaltosaar/papers/mendeley collection/Pennington, Manning - Unknown - Glove Global vectors for word representation.pdf:pdf},
journal = {Emnlp2014.Org},
title = {{Glove: Global vectors for word representation}},
url = {http://emnlp2014.org/papers/pdf/EMNLP2014162.pdf}
}
@article{Dinu,
author = {Dinu, Georgiana and Baroni, Marco},
file = {:Users/jaanaltosaar/papers/mendeley collection/Dinu, Baroni - Unknown - How to make words with vectors Phrase generation in distributional semantics.pdf:pdf},
title = {{How to make words with vectors : Phrase generation in distributional semantics}},
url = {http://clic.cimec.unitn.it/marco/publications/acl2014/dinu-baroni-generation-acl2014.pdf}
}
@article{Ma2015,
abstract = {In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC.},
archivePrefix = {arXiv},
arxivId = {1507.01839},
author = {Ma, Mingbo and Huang, Liang and Zhou, Bowen and Xiang, Bing},
eprint = {1507.01839},
file = {:Users/jaanaltosaar/papers/mendeley collection/Ma et al. - 2015 - Tree-based Convolution for Sentence Modeling.pdf:pdf},
number = {1995},
title = {{Tree-based Convolution for Sentence Modeling}},
url = {http://arxiv.org/abs/1507.01839},
year = {2015}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.4053},
author = {Le, Q V and Mikolov, Tomas},
eprint = {arXiv:1405.4053},
file = {:Users/jaanaltosaar/papers/mendeley collection/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents.pdf:pdf},
isbn = {9781634393973},
journal = {ICML},
pages = {1188--1196},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Mnih2013,
abstract = {Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information aboutwords very well, setting performance records on severalword similarity tasks. The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor. We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation. Our approach is simpler, faster, and produces better results than the current state-of-the- art method. We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of mag- nitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.},
author = {Mnih, Andriy},
file = {:Users/jaanaltosaar/papers/mendeley collection/Mnih - 2013 - Learning word embeddings efficiently with noise-contrastive estimation.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Learning word embeddings efficiently with noise-contrastive estimation}},
url = {http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf},
year = {2013}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:Users/jaanaltosaar/papers/mendeley collection/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
url = {http://jmlr.csail.mit.edu/papers/volume3/bengio03a/bengio03a.pdf},
volume = {3},
year = {2003}
}
@article{Dai2015,
abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.07998},
author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
eprint = {arXiv:1507.07998},
file = {:Users/jaanaltosaar/papers/mendeley collection/Dai, Olah, Le - 2015 - Document Embedding with Paragraph Vectors.pdf:pdf},
pages = {1--8},
title = {{Document Embedding with Paragraph Vectors}},
url = {http://arxiv.org/abs/1507.07998},
year = {2015}
}
@article{Mnih2007,
abstract = {The current study characterized the in vitro surface reactions of microroughened bioactive glasses and compared osteoblast cell responses between smooth and microrough surfaces. Three different bioactive glass compositions were used and surface microroughening was obtained using a novel chemical etching method. Porous bioactive glass specimens made of sintered microspheres were immersed in simulated body fluid (SBF) or Tris solutions for 1, 6, 24, 48, or 72 h, and the formation of reaction layers was studied by means of a scanning electron microscope/energy dispersive X-ray analysis (SEM/EDXA). Cell culture studies were performed on bioactive glass disks to examine the influence of surface microroughness on the attachment and proliferation of human osteoblast-like cells (MG-63). Cell attachment was evaluated by means of microscopic counting of in situ stained cells. Cell proliferation was analyzed with a nonradioactive cell proliferation assay combined with in situ staining and laser confocal microscopy. The microroughening of the bioactive glass surface increased the rate of the silica gel layer formation during the first hours of the immersion. The formation of calcium phosphate layer was equal between control and microroughened glass surfaces. In cell cultures on bioactive glass, the microrough surface enhanced the attachment of osteoblast-like cells but did not have an effect on the proliferation rate or morphology of the cells as compared with smooth glass surface. In conclusion, accelerated the early formation of surface reactions on three bioactive glasses and had a positive effect on initial cell attachment.},
author = {Mnih, A and Hinton, G},
doi = {10.1145/1273496.1273577},
file = {:Users/jaanaltosaar/papers/mendeley collection/Mnih, Hinton - 2007 - Three new graphical models for statistical language modelling.pdf:pdf},
isbn = {9781595937933},
journal = {ICML},
pages = {641--648},
title = {{Three new graphical models for statistical language modelling.}},
url = {http://discovery.ucl.ac.uk/63252/},
volume = {62},
year = {2007}
}
@article{Levy,
abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word simi-larity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
author = {Levy, Omer and Goldberg, Yoav},
file = {:Users/jaanaltosaar/papers/mendeley collection/Levy, Goldberg - 2014 - Neural Word Embedding as Implicit Matrix Factorization.pdf:pdf},
journal = {NIPS},
pages = {1--9},
title = {{Neural Word Embedding as Implicit Matrix Factorization}},
url = {http://u.cs.biu.ac.il/{~}nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf},
year = {2014}
}
@article{Levy2015a,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
file = {:Users/jaanaltosaar/papers/mendeley collection/Levy, Goldberg, Dagan - 2015 - Improving Distributional Similarity with Lessons Learned from Word Embeddings.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the ACL},
pages = {211--225},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {http://aclweb.org/anthology/Q/Q15/Q15-1016.pdf},
volume = {3},
year = {2015}
}
@article{Huang2016,
annote = {@inproceedings{\{}Huang:2016:CRC:2856767.2856792,
author = {\{}Huang, Cheng-Zhi Anna and Duvenaud, David and Gajos, Krzysztof Z.{\}},
title = {\{}ChordRipple: Recommending Chords to Help Novice Composers Go Beyond the Ordinary{\}},
booktitle = {\{}Proceedings of the 21st International Conference on Intelligent User Interfaces{\}},
series = {\{}IUI '16{\}},
year = {\{}2016{\}},
isbn = {\{}978-1-4503-4137-0{\}},
location = {\{}Sonoma, California, USA{\}},
pages = {\{}241--250{\}},
numpages = {\{}10{\}},
url = {\{}http://doi.acm.org/10.1145/2856767.2856792{\}},
doi = {\{}10.1145/2856767.2856792{\}},
acmid = {\{}2856792{\}},
publisher = {\{}ACM{\}},
address = {\{}New York, NY, USA{\}},
keywords = {\{}chords, creativity support tools, recommender systems, neural language models, embeddings, harmony, music, songwriting{\}},
{\}}},
author = {Huang, Cheng-Zhi Anna and Duvenaud, David and Gajos, Krzysztof Z.},
file = {:Users/jaanaltosaar/papers/mendeley collection/Huang, Duvenaud, Gajos - 2015 - ChordRipple Recommending Chords to Help Novice Composers Go Beyond Themselves.pdf:pdf},
isbn = {9781450341370},
pages = {241--250},
title = {{ChordRipple : Recommending Chords to Help Novice Composers Go Beyond Themselves}},
url = {http://delivery.acm.org/10.1145/2860000/2856792/p241-huang.pdf?ip=184.152.36.98{\&}id=2856792{\&}acc=OPEN{\&}key=4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35.6D218144511F3437{\&}CFID=762476854{\&}CFTOKEN=97662156{\&}{\_}{\_}acm{\_}{\_}=1458225391{\_}f5b4a3ff3f3194266f8455ea636e28cd},
year = {2015}
}
@article{Asgari2015,
abstract = {We propose a new approach for representing biological sequences. This method, named protein-vectors or ProtVec for short, can be utilized in bioinformatics applications such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. Using the Skip-gram neural networks, protein sequences are represented with a single dense n-dimensional vector. This method was evaluated by classifying protein sequences obtained from Swiss-Prot belonging to 7,027 protein families where an average family classification accuracy of {\$}94\backslash{\%}\backslashpm 0.03\backslash{\%}{\$} was obtained, outperforming existing family classification methods. In addition, our model was used to predict disordered proteins from structured proteins. Two databases of disordered sequences were used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences were distinguished from structured Protein Data Bank (PDB) sequences with 99.81$\backslash${\%} accuracy, and unstructured DisProt sequences from structured DisProt sequences with 100.0$\backslash${\%} accuracy. These results indicate that by only providing sequence data for various proteins into this model, information about protein structure can be determined with high accuracy. This so-called embedding model needs to be trained only once and can then be used to ascertain a diverse set of information regarding the proteins of interest. In addition, this representation can be considered as pre-training for various applications of deep learning in bioinformatics.},
archivePrefix = {arXiv},
arxivId = {1503.05140},
author = {Asgari, Ehsaneddin and Mofrad, Mohammad R K},
doi = {10.1371/journal.pone.0141287},
eprint = {1503.05140},
file = {:Users/jaanaltosaar/papers/mendeley collection/Asgari, Mofrad - 2015 - Continuous distributed representation of biological sequences for deep proteomics and genomics.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--15},
pmid = {26555596},
title = {{Continuous distributed representation of biological sequences for deep proteomics and genomics}},
url = {http://journals.plos.org/plosone/article/asset?id=10.1371/journal.pone.0141287.PDF},
volume = {10},
year = {2015}
}


@inproceedings{huang_chordripple_2016,
  address = {New York, NY, USA},
  series = {IUI '16},
  title = {{{ChordRipple}}: {{Recommending Chords}} to {{Help Novice Composers Go Beyond}} the {{Ordinary}}},
  isbn = {978-1-4503-4137-0},
  shorttitle = {{{ChordRipple}}},
  doi = {10.1145/2856767.2856792},
  abstract = {Novice composers often find it difficult to go beyond common chord progressions. To make it easier for composers to experiment with radical chord choices, we built a creativity support tool, ChordRipple, which makes chord recommendations that aim to be both diverse and appropriate to the current context. Composers can use it to help select the next chord, or to replace sequences of chords in an internally consistent manner. To make such recommendations, we adapt a neural network model from natural language processing known as Word2Vec to the music domain. This model learns chord embeddings from a corpus of chord sequences, placing chords nearby when they are used in similar contexts. The learned embeddings support creative substitutions between chords, and also exhibit topological properties that correspond to musical structure. For example, the major and minor chords are both arranged in the latent space in shapes corresponding to the circle-of-fifths. Our structured observations with 14 music students show that the tool helped them explore a wider palette of chords, and to make "big jumps in just a few chords". It gave them "new ideas of ways to move forward in the piece", not just on a chord-to-chord level but also between phrases. Our controlled studies with 9 more music students show that more adventurous chords are adopted when composing with ChordRipple.},
  timestamp = {2016-03-11T15:30:29Z},
  urldate = {2016-03-11},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Intelligent User Interfaces}}},
  publisher = {{ACM}},
  author = {Huang, Cheng-Zhi Anna and Duvenaud, David and Gajos, Krzysztof Z.},
  year = {2016},
  keywords = {chords,creativity support tools,embeddings,harmony,Music,neural language models,recommender systems,songwriting},
  pages = {241--250},
  file = {Huang et al. - 2016 - ChordRipple Recommending Chords to Help Novice Co.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/8RP4E7ZI/Huang et al. - 2016 - ChordRipple Recommending Chords to Help Novice Co.pdf:application/pdf}
}

@article{jacoby_information_2015,
  title = {An {{Information Theoretic Approach}} to {{Chord Categorization}} and {{Functional Harmony}}},
  volume = {44},
  issn = {0929-8215, 1744-5027},
  doi = {10.1080/09298215.2015.1036888},
  language = {en},
  timestamp = {2016-03-11T17:13:31Z},
  number = {3},
  urldate = {2016-03-11},
  journal = {Journal of New Music Research},
  author = {Jacoby, Nori and Tishby, Naftali and Tymoczko, Dmitri},
  month = jul,
  year = {2015},
  pages = {219--244},
  file = {Jacoby et al. - 2015 - An Information Theoretic Approach to Chord Categor.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/EPITWTMN/Jacoby et al. - 2015 - An Information Theoretic Approach to Chord Categor.pdf:application/pdf}
}

@article{kottur_visual_2015,
  title = {Visual {{Word2Vec}} (vis-w2v): {{Learning Visually Grounded Word Embeddings Using Abstract Scenes}}},
  shorttitle = {Visual {{Word2Vec}} (vis-w2v)},
  abstract = {We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, visual grounding can help us realize that concepts like eating and staring at are related, since when people are eating something, they also tend to stare at the food. Grounding a rich variety of relations like eating and stare at in vision is a challenging task, despite recent progress in vision. We realize the visual grounding for words depends on the semantics of our visual world, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained visually grounded notions of semantic relatedness. We show improvements over text only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets will be available online.},
  timestamp = {2016-03-11T17:27:47Z},
  urldate = {2016-03-11},
  journal = {arXiv:1511.07067 {[}cs]},
  author = {Kottur, Satwik and Vedantam, Ramakrishna and Moura, Jos{\'e} M. F. and Parikh, Devi},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {arXiv.org Snapshot:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/K3A6NSJH/1511.html:;arXiv\:1511.07067 PDF:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/ZA4E998I/Kottur et al. - 2015 - Visual Word2Vec (vis-w2v) Learning Visually Groun.pdf:application/pdf},
  arxiv = {1511.07067}
}

@inproceedings{nichols_data-driven_2009,
  title = {Data-driven exploration of musical chord sequences},
  timestamp = {2016-03-11T15:40:59Z},
  urldate = {2016-03-11},
  booktitle = {Proceedings of the 14th international conference on {{Intelligent}} user interfaces},
  publisher = {{ACM}},
  author = {Nichols, Eric and Morris, Dan and Basu, Sumit},
  year = {2009},
  pages = {227--236},
  file = {Nichols et al. - 2009 - Data-driven exploration of musical chord sequences.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/3Z573PAU/Nichols et al. - 2009 - Data-driven exploration of musical chord sequences.pdf:application/pdf}
}

@article{raphael_functional_2004,
  title = {Functional {{Harmonic Analysis Using Probabilistic Models}}},
  volume = {28},
  issn = {0148-9267},
  doi = {10.1162/0148926041790676},
  timestamp = {2016-03-11T17:17:49Z},
  number = {3},
  urldate = {2016-03-11},
  journal = {Computer Music Journal},
  author = {Raphael, Christopher and Stoddard, Joshua},
  month = sep,
  year = {2004},
  pages = {45--52},
  file = {Computer Music Journal Snapshot:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/NQTZS26U/0148926041790676.html:;Computer Music Journal Full Text PDF:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/XNXMIAMW/Raphael and Stoddard - 2004 - Functional Harmonic Analysis Using Probabilistic M.pdf:application/pdf}
}

@misc{rong_word2vec_????,
  title = {word2vec {{Parameter Learning Explained}}},
  timestamp = {2016-03-11T17:23:29Z},
  author = {Rong, Xin},
  file = {w2vexp.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/BQDJZHFM/w2vexp.pdf:application/pdf}
}

@article{silberer_learning_2015,
  title = {Learning visually grounded meaning representations},
  timestamp = {2016-03-11T17:27:08Z},
  urldate = {2016-03-11},
  author = {Silberer, Carina Helga},
  year = {2015},
  file = {Silberer - 2015 - Learning visually grounded meaning representations.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/C6N9CKFT/Silberer - 2015 - Learning visually grounded meaning representations.pdf:application/pdf}
}

@article{temperley_statistical_2013,
  title = {Statistical {{Analysis}} of {{Harmony}} and {{Melody}} in {{Rock Music}}},
  volume = {42},
  issn = {0929-8215, 1744-5027},
  doi = {10.1080/09298215.2013.788039},
  language = {en},
  timestamp = {2016-03-11T17:16:53Z},
  number = {3},
  urldate = {2016-03-11},
  journal = {Journal of New Music Research},
  author = {Temperley, David and {de Clercq}, Trevor},
  month = sep,
  year = {2013},
  pages = {187--204},
  file = {Temperley and Clercq - 2013 - Statistical Analysis of Harmony and Melody in Rock.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/S53PF23K/Temperley and Clercq - 2013 - Statistical Analysis of Harmony and Melody in Rock.pdf:application/pdf}
}

@inproceedings{white2014yale,
  title = {The {{Yale}}-classical archives corpus},
  timestamp = {2016-03-11T17:11:18Z},
  booktitle = {Poster presented at: {{International Conference}} for {{Music Perception}} and {{Cognition}}. {{Seoul}}, {{South Korea}}},
  author = {White, C and Quinn, I},
  year = {2014}
}

@article{white_statistical_2011,
  title = {Some {{Statistical Properties}} of {{Roman Numerals}}},
  timestamp = {2016-03-11T17:13:37Z},
  urldate = {2016-03-11},
  author = {White, Christopher},
  year = {2011},
  file = {White - 2011 - Some Statistical Properties of Roman Numerals.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/KTD99WTR/White - 2011 - Some Statistical Properties of Roman Numerals.pdf:application/pdf}
}

@incollection{collins_corpus-sensitive_2015,
  address = {Cham},
  title = {A {{Corpus-Sensitive Algorithm}} for {{Automated Tonal Analysis}}},
  volume = {9110},
  isbn = {978-3-319-20602-8 978-3-319-20603-5},
  timestamp = {2016-03-11T17:13:34Z},
  urldate = {2016-03-11},
  booktitle = {Mathematics and {{Computation}} in {{Music}}},
  publisher = {{Springer International Publishing}},
  author = {White, Christopher Wm.},
  editor = {Collins, Tom and Meredith, David and Volk, Anja},
  year = {2015},
  pages = {115--121},
  file = {White - 2015 - A Corpus-Sensitive Algorithm for Automated Tonal A.pdf:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/M9F4SVE7/White - 2015 - A Corpus-Sensitive Algorithm for Automated Tonal A.pdf:application/pdf}
}

@inproceedings{rehurek_lrec,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      language={English}
}

@inproceedings{Hu2009,
  title={A Probabilistic Topic Model for Unsupervised Learning of Musical Key-Profiles.},
  author={Hu, Diane and Saul, Lawrence K},
  booktitle={ISMIR},
  pages={441--446},
  year={2009},
  organization={Citeseer}
}


@article{Arora2016,
  title = {Linear {{Algebraic Structure}} of {{Word Senses}}, with {{Applications}} to {{Polysemy}}},
  abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it's unclear what they represent when the word is polysemous, i.e., has multiple senses. Here it is shown that multiple word senses reside in linear superposition within the word embedding and can be recovered by simple sparse coding. The success of the method ---which applies to several embedding methods including word2vec--- is mathematically explained using the random walk on discourses model (Arora et al., 2015). A novel aspect of our technique is that each word sense is also accompanied by one of about 2000 "discourse atoms" that give a succinct description of which other words co-occur with that word sense. Discourse atoms seem of independent interest, and make the method potentially more useful than the traditional clustering-based approaches to polysemy.},
  timestamp = {2016-03-23T04:19:52Z},
  urldate = {2016-03-23},
  journal = {arXiv:1601.03764 [cs, stat]},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  month = jan,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Statistics - Machine Learning},
  file = {arXiv.org Snapshot:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/C652B8RJ/1601.html:;arXiv\:1601.03764 PDF:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/JXKDIK8G/Arora et al. - 2016 - Linear Algebraic Structure of Word Senses, with Ap.pdf:application/pdf},
  arxiv = {1601.03764}
}

@ARTICLE{Ponsford99,
    author = {Dan Ponsford and Geraint Wiggins and Chris Mellish},
    title = {Statistical Learning of Harmonic Movement},
    journal = {Journal of New Music Research},
    year = {1999},
    volume = {28},
    pages = {150--177}
}


@article{Callender2008,
  title = {Generalized voice-leading spaces},
  volume = {320},
  timestamp = {2016-03-24T02:46:35Z},
  number = {5874},
  journal = {Science},
  author = {Callender, Clifton and Quinn, Ian and Tymoczko, Dmitri},
  year = {2008},
  pages = {346--348}
}

@article{Lewin1959,
  title = {Re: {{Intervallic Relations}} between {{Two Collections}} of {{Notes}}},
  volume = {3},
  issn = {0022-2909},
  shorttitle = {Re},
  doi = {10.2307/842856},
  timestamp = {2016-03-24T02:41:30Z},
  number = {2},
  urldate = {2016-03-24},
  journal = {Journal of Music Theory},
  author = {Lewin, David},
  year = {1959},
  pages = {298--301}
}

@inproceedings{White2013,
  address = {Charlotte, NC},
  title = {Expanding {{Notions}} of {{Harmonic Function Through}} a {{Corpus Analysis}} of the {{Bach Chorales}}},
  timestamp = {2016-03-24T02:51:52Z},
  author = {White, Christopher and Quinn, Ian},
  year = {2013}
}

@article{Quinn2006,
  title = {General {{Equal-Tempered Harmony}} ({{Introduction}} and {{Part I}})},
  volume = {44},
  issn = {0031-6016},
  timestamp = {2016-03-24T02:45:34Z},
  number = {2},
  urldate = {2016-03-24},
  journal = {Perspectives of New Music},
  author = {Quinn, Ian},
  year = {2006},
  pages = {114--158}
}

@article{Yust2015,
  title = {Schubert's {{Harmonic Language}} and {{Fourier Phase Space}}},
  volume = {59},
  issn = {0022-2909,},
  doi = {10.1215/00222909-2863409},
  abstract = {This article introduces a type of harmonic geometry, Fourier phase space, and uses it to advance the understanding of Schubert's tonal language and comment upon current topics in Schubert analysis. The space derives from the discrete Fourier transform on pitch-class sets developed by David Lewin and Ian Quinn but uses primarily the phases of Fourier components, unlike Lewin and Quinn, who focus more on the magnitudes. The space defined by phases of the third and fifth components closely resembles the Tonnetz and has a similar common-tone basis to its topology but is continuous and takes a wider domain of harmonic objects. A number of musical examples show how expanding the domain enables us to extend and refine some the conclusions of neo-Riemannian theory about Schubert's harmony. Through analysis of the Trio and Adagio from Schubert's String Quintet and other works using the geometry, the article develops a number of concepts for the analysis of chromatic harmony, including a geometric concept of interval as direction (intervallic axis), a novel approach to triadic voice leading (triadic orbits), and theories of tonal regions.},
  language = {en},
  timestamp = {2016-03-24T02:45:57Z},
  number = {1},
  urldate = {2016-03-24},
  journal = {Journal of Music Theory},
  author = {Yust, Jason},
  month = jan,
  year = {2015},
  keywords = {chromatic harmony,Fourier transform,Schubert,Tonnetz,topology},
  pages = {121--181},
  file = {Snapshot:/home/eamonn/.mozilla/firefox/g0rg91vj.default/zotero/storage/29WAA7PZ/121.html:}
}

@article{Wolf2014,
  title={Joint word2vec networks for bilingual semantic representations},
  author={Wolf, Lior and Hanani, Yair and Bar, Kfir and Dershowitz, Nachum},
  journal={International Journal of Computational Linguistics and Applications},
  volume={5},
  number={1},
  pages={27--44},
  year={2014}
}

@inproceedings{Shanahan2013,
  address = {Toronto},
  title = {The {{Acquisition}} and {{Validation}} of {{Large Web-Based Corpora}}},
  timestamp = {2016-03-24T04:10:48Z},
  author = {Shanahan, Daniel and Albrecht, Joshua},
  month = aug,
  year = {2013}
}

