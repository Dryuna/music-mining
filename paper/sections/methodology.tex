%!TEX root = ../ycac2vec.tex

\subsection{Corpus}\label{corpus}

The Yale/Classical Archives Corpus (YCAC) is a database of pitch-class
and time data from MIDI files contributed by users of
classicalarchives.com encoding 8,980 distinct pieces of music
\cite{white2014yale}. Each piece is represented both by a sequence of
time-coded \texttt{music21.Chord} objects which were obtained by ``salami slicing'' the original MIDI file, view of the piece in which a new chord token is created at every moment a voice enters or leaves the musical texture. 

To restrict the total vocabulary of the word embedding models to be applied later, we reduce each chord token to a binarized chroma vector. A binarized chroma vector is a twelve-place vector $v \in {\{0,1\}}^{12}$ in which the $i$-th component of vector $v_i$ is set to $1$ if the $i$-th pitch class is present in the chord token. This choice also reflects the general programmatic interest of music theory in generalizing about rules of harmonic progression based on pitch content.

A subset of the corpus was divided into four comparably-sized subcorpora based on the dates of composition of pieces, in fifty-year chunks from 1700--1899 (Table \ref{tab:counts}). Where an exact date of composition was not found in the piece metadata provided with the YCAC, the midpoint of the date range given in the metadata was used. Typically, this range was the lifespan of the composer.

\begin{table}
 \begin{center}
 \begin{tabular}{|l|l|l|}
  \hline
  Year of composition & No. pieces & No. slices \\
  & (documents) & (chord tokens) \\
\hline
\hline
1700--1749 & 2 120 & 1 892 572 \\
\hline
1750--1799 & 1 806 & 2 820 522 \\
\hline
1800--1849 & 1 770 & 3 565 872 \\
\hline
1850--1899 & 1 601 & 2 635 754 \\
\hline

 \end{tabular}
\end{center}
 \caption{Piece (document) and slice (token) counts in the time-delimited subcorpora.}
 \label{tab:counts}
\end{table}

\subsection{Embedding space}\label{embedding-space}

A word embedding model in the word2vec family was used to learn a number of embedding
spaces. First, a space was trained on the entire corpus to evaluate its plausibility. Then, four separate word-embedding spaces were trained on the date-delimited subcorpora described above.\footnote{The implementation used was the word2vec model provided by the Python module `gensim', which uses a skip-gram negative sampling (SGNS) model which has been shown to be effective on large textual corpora \cite{rehurek_lrec}. The model was instantiated with the following invocation: \texttt{gensim.models.Word2Vec(sentences, size=100, window=5, min\_count=5, workers=4, sg=1)}}
This algorithm treats each chroma vector as a token, and every piece as a sequence of chroma vectors. It returns a 100-dimensional real-valued vector for each chroma vector. Dimensionality reduction was applied to whole corpus embedding space to evaluate the plausibility of the embedding space. Dimensionality reduction was applied to the subcorpora embedding spaces to evaluate the, and the locations of chroma vectors corresponding to major triads were plotted in each embedding space.