%!TEX root = ../ycac2vec.tex

\subsection{Word embeddings}
% lit review on word embeddings
Probabilistic models such as Latent Dirichlet Allocation \cite{Blei2003} are standard tools for analyzing text data. However, such models use bag-of-words representations. Therefore they extract meaning from word co-occurrence counts on the document level, and ignore the sequential nature of language. Syntax, punctuation, and grammar are intrinsically sequential, and a good model of natural language should be able to capture both information from co-occurrence counts and information across time. While Latent Dirichlet Allocation has been used to model key profiles as topics in tonal music \cite{Hu2009}, the bag-of-words approach to musical corpora is limited, since music is also a sequential phenomenon. For in-depth analysis of style in music, we turn to word embedding models.

Word embeddings, or real-valued vectors representing words in a vocabulary, were first introduced by \cite{Bengio2003} but popularized by \cite{Mikolov2013a} under the `word2vec' moniker. Such models typically have a log-bilinear form \cite{Mnih2007}, and are trained using negative sampling with a fixed-size context window \cite{Mikolov2013a}. This is equivalent to matrix factorization of a shifted pointwise mutual information word-context matrix \cite{Levy}. 

To see this, construct a co-occurrence matrix of words, where the rows and columns are the words in the vocabulary. Each entry $i, j$ in the matrix is the count of how many times word $i$ (e.g. `dog') occurred in the context of word $j$ (e.g. `the'). Word embedding models such as the skip-gram model we use in the present paper can be viewed as performing singular value decomposition on a transformed version of this matrix. The embedding space returned by a model trained on a sufficiently large and informative corpus models a notion of semantic similarity as cosine distance between word vectors.

Compositional word embeddings for learning paragraph or document embeddings have also been proposed \cite{Le2014,Dai2015}. However, \cite{Levy2015a} suggests that much of the success of these types of distributed representations of words is due to the tricks needed to train such models such as noise contrastive estimation. \cite{Arora2015} and \cite{Arora2016} show that word embedding models can also be thought of as random walks in the embedding space. 


\subsection{Statistical analysis of harmony}

Attempts to generalize rules describing the use of harmony by composers of Western art music (hereafter, classical music) have been characteristic of music theory for centuries. For instance traditional functional harmonic theory classifies chords into three categories: tonic, dominant, and subdominant. 

These classifications are based on expert empirical knowledge of music, and can be used to prescribe rules and/or prototypes for sequences of musical harmonies. For example, the progression
$$\textrm{tonic} \xrightarrow{} \textrm{subdominant} \xrightarrow{} \textrm{dominant} \xrightarrow{} \textrm{tonic}$$
is normative in classical music. The availability of increasingly larger and more representative symbolic corpora of musical works allows for the statistical exploration of the basis for these categorization schemes.

\cite{nichols_data-driven_2009, temperley_statistical_2013} use finite state-transition probability matrices to characterize harmonic progressions in music in diverse styles. \cite{Ponsford99} uses a state-transition model to learn plausible harmonic progressions from a small corpus of 17th-century dance, while \cite{collins_corpus-sensitive_2015} outlines a state-transition approach relevant to a large classical music corpus. More recently, \cite{White2013} uses unsupervised machine learning to discover alternatives to the three-category classification scheme of traditional functional harmony.

\subsection{Prospects for musical embedding models}

% lit review on word embeddings applied to things other than natural language
As useful models of discrete data, word embedding models are starting to be used in domains outside of natural language. For example, \cite{Asgari2015} embed protein sequences for classification, and \cite{Guardia-Sebaoun2015} develop an embedding model to build a recommendation system (for example, for recommending movies to users). There exists some prior work on applying word embedding models to music. \cite{Huang2016} trained a word embedding model on a much smaller (n = 200) corpus of pop and rock music in service of a novel user interface in computer-aided composition environment. 

Our approach trains a similar word embedding model, but on a significantly larger corpus. Word embedding models in the word2vec family have shown robust results on natural language reasoning tasks when trained on corpora of the order of a million tokens. At 12M chord tokens, Yale/Classical Archives Corpus (YCAC) introduced by \cite{white2014yale} approaches the size thought to be required for this technique. Despite the fact that \cite{Shanahan2013} show high pitch-class error rates in this corpus compared to the scores its contents purport to transmit, the YCAC remains the most acessible corpus of classical music available at the scale thought to be useful for word embedding models. Our goal is to explore the resulting space for latent structures analogous to those which allow for thes utility of these models in NLP tasks. 

\cite{jacoby_information_2015} usefully characterize theories of harmony such as traditional functional harmony in the most general terms as mappings from an annotated corpus to a list of categories and provide information-theoretical criteria for evaluating these kinds of theories, including unseen or novel theories of harmony. Our results suggest that the structure of word embedding spaces trained on musical data captures some intuitions about harmonic function. Both traditional and topological clustering on the representatives in the embedding space might be effectively applied to these spaces. Such clustering algorithms implicitly define categorization functions that, in turn, may be evaluated according to the method in \cite{jacoby_information_2015}.

