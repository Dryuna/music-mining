%!TEX root = ../ycac2vec.tex

\subsection{Word embeddings}
% lit review on word embeddings
Probabilistic models such as Latent Dirichlet Allocation \cite{Blei2003} are standard tools for analyzing text data. However, such models use bag-of-words representations. Therefore they extract meaning from word co-occurrance counts on the document level, and ignore the sequential nature of language. Syntax, punctuation, and grammar are intrinsically sequential, and a good model of natural language or tree structures should be able to capture both information from co-occurrance counts and information across time. While Latent Dirichlet Allocation has been used to model key profiles as topics in tonal music \cite{Hu2009}, the bag-of-words approach to musical corpora is limitied, since music is inherently sequential. For in-depth analysis of style in music, we need embedding models.

Word embeddings, or real-valued vectors representing words in a vocabulary, were first introduced by \cite{Bengio2003} but popularized by \cite{Mikolov2013a}. Such models typically have a log-bilinear form \cite{Mnih2007}, and are trained using negative sampling with a fixed size context window \cite{Mikolov2013a}. This is equivalent to matrix factorization of a shifted pointwise mutual information word-context matrix \cite{Levy}. To see this, construct a co-occurrance matrix of words, where the rows and columns are the words in the vocabulary. Each entry $i, j$ in the matrix is the count of how many times word $i$ (e.g. `dog') ocurred in the context of word $j$ (e.g. `the'). Word embedding models such as the skip-gram model (word2vec) can be viewed as performing singular value decomposition on a transformed version of this matrix.  Compositional word embeddings for learning paragraph or document embeddings have also been proposed \cite{Le2014,Dai2015}. However, \cite{Levy2015a} suggests that much of the success of these types of distributed representations of words is due to the tricks needed to train such models such as noise contrastive estimation. \cite{Arora2015} and \cite{Arora2016} show that word embedding models can also be thought of as random walks in the embedding space.

% lit review on word embeddings applied to things other than natural language
As useful models of discrete data, word embedding models are starting to be used in domains outside of natural language. For example, \cite{Asgari2015} embed protein sequences for classification, and \cite{Guardia-Sebaoun2015} develop an embedding model to build a recommendation system (for example, for recommending movies to users). There exists some prior work on applying word embedding models to music. \cite{Huang2016} trained an embedding model on a corpus of 200 rock songs for the task of recommendating chords to composers.

\subsection{Statistical analysis of harmony}

Attempts to generalize rules describing the use of harmony by composers have been characteristic of music theory for centuries. Scale-degree theory classifies triads and other important chords according to the scale degree of the root of the chord, with respect to the local key area. Traditional functional harmony classifies chords into three categories: tonic, dominant, and subdominant. These categories are based on expert knowledge of 

\cite{nichols_data-driven_2009, temperley_statistical_2013} use finite state-transition probability matrices to characterize harmonic progressions in music in diverse styles. \cite{Ponsford99} uses a state-transition model to learn plausible harmonic progressions from a small corpus of 17th-century dance, while \cite{collins_corpus-sensitive_2015} outlines a state-transition approach relevant to a large classical music corpus. Hidden Markov Models have been recently used to propose alternatives to the three-category classification scheme of traditional functional harmony. \cite{jacoby_information_2015} usefully characterize theories of harmony such as scale-degree theory and functional harmony in the most general terms as mappings from an annotated corpus to a list of categories and provide information-theoretical criteria for evaluating these kinds of theories, including unseen or novel theories of harmony.

Word embedding models in the word2vec family have shown robust results on natural language reasoning tasks when trained on corpora of the order of a million tokens. The Yale/Classical Archives corpus approaches the size thought to be required for this technique. Therefore, the application of word embedding to this particular corpus is an apt first step in exploring the. \cite{Huang2016} deploys word embedding models trained on a much smaller corpus in service of a novel user interface in computer-aided composition environment. By contrast, our approach trains a similar word embedding model, but on a significantly larger corpus with the different goal of exploring the resulting space for latent structures analogous to those which allow for the remarkable utility of these models in NLP tasks. 

It is important to note that here we do not propose any particular categorization function. Nevertheless, our results suggest that the structure of word embedding spaces trained on musical data captures some intuitions about harmonic function. Both traditional and topological clustering on the representatives in the embedding space might be effectively applied to these spaces. Such clustering algorithms would then generate categorizations that, in turn, may be evaluated after \cite{jacoby_information_2015}.

